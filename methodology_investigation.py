"""
Meta Signals Methodology Investigation

Reverse engineer how Meta Signals are generated by analyzing entry conditions,
technical indicators, and market context at signal times.
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
import sys

# Add src to path
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, 'src')
sys.path.insert(0, src_dir)

from data.binance_data import BinanceDataFetcher

class SignalMethodologyInvestigator:
    """Investigate how Meta Signals are generated"""
    
    def __init__(self, signals_file: str = None):
        """Initialize with signals data"""
        if signals_file is None:
            # Find latest signals file
            signals_dir = "data/signals"
            files = [f for f in os.listdir(signals_dir) if f.endswith('.csv')]
            if files:
                signals_file = os.path.join(signals_dir, sorted(files)[-1])
        
        if signals_file and os.path.exists(signals_file):
            self.signals_df = pd.read_csv(signals_file)
            print(f"ğŸ“Š Loaded {len(self.signals_df)} signals from {os.path.basename(signals_file)}")
        else:
            self.signals_df = pd.DataFrame()
            print("âŒ No signals file found.")
        
        self.binance = BinanceDataFetcher()
        self.analysis_results = {}
    
    def analyze_entry_conditions(self, sample_size: int = 20):
        """Analyze market conditions at signal entry points"""
        print("\\nğŸ¯ ENTRY CONDITIONS ANALYSIS")
        print("=" * 50)
        
        if self.signals_df.empty:
            return
        
        # Sample signals for analysis
        sample_signals = self.signals_df.head(sample_size)
        entry_conditions = []
        
        print(f"ğŸ” Analyzing entry conditions for {sample_size} signals...")
        
        for idx, signal in sample_signals.iterrows():
            try:
                symbol = signal['symbol'] + 'USDT'
                signal_time = pd.to_datetime(signal['timestamp'])
                entry_price = float(signal['entry_price'])
                
                # Get data around signal time (1 hour before to 1 hour after)
                start_time = signal_time - timedelta(hours=1)
                end_time = signal_time + timedelta(hours=1)
                
                df = self.binance.get_kline_data(symbol, start_time, end_time, interval="1m")
                
                if df.empty:
                    continue
                
                # Find closest candle to signal time
                df['time_diff'] = abs(df['timestamp'] - signal_time)
                closest_idx = df['time_diff'].idxmin()
                signal_candle = df.loc[closest_idx]
                
                # Get previous candles for context
                candle_idx = df.index.get_loc(closest_idx)
                if candle_idx >= 10:  # Need at least 10 previous candles
                    
                    # Calculate technical indicators
                    recent_data = df.iloc[candle_idx-10:candle_idx+1]
                    
                    # Price analysis
                    current_price = signal_candle['close']
                    price_diff = abs(current_price - entry_price)
                    price_accuracy = (1 - price_diff / entry_price) * 100
                    
                    # Moving averages
                    ma5 = recent_data['close'].tail(5).mean()
                    ma10 = recent_data['close'].tail(10).mean()
                    
                    # Volume analysis
                    avg_volume = recent_data['volume'].mean()
                    current_volume = signal_candle['volume']
                    volume_ratio = current_volume / avg_volume
                    
                    # Price momentum
                    price_change_5min = (current_price - recent_data['close'].iloc[-6]) / recent_data['close'].iloc[-6] * 100
                    price_change_10min = (current_price - recent_data['close'].iloc[-11]) / recent_data['close'].iloc[-11] * 100
                    
                    # Volatility
                    price_range = (signal_candle['high'] - signal_candle['low']) / signal_candle['close'] * 100
                    
                    entry_condition = {
                        'symbol': signal['symbol'],
                        'signal_time': signal_time,
                        'entry_price': entry_price,
                        'actual_price': current_price,
                        'price_accuracy': price_accuracy,
                        'above_ma5': current_price > ma5,
                        'above_ma10': current_price > ma10,
                        'ma5_above_ma10': ma5 > ma10,
                        'volume_ratio': volume_ratio,
                        'momentum_5min': price_change_5min,
                        'momentum_10min': price_change_10min,
                        'price_range': price_range,
                        'timeframe': signal.get('timeframe', 'Unknown'),
                        'strategy_version': signal.get('strategy_version', 'Unknown')
                    }
                    
                    entry_conditions.append(entry_condition)
                    
                    print(f"  {signal['symbol']:6} | Price Accuracy: {price_accuracy:5.1f}% | " +
                          f"Above MA5: {current_price > ma5} | Volume: {volume_ratio:4.1f}x | " +
                          f"Momentum: {price_change_5min:+5.1f}%")
                
            except Exception as e:
                print(f"    âŒ Error analyzing {signal['symbol']}: {e}")
                continue
        
        # Analyze patterns
        if entry_conditions:
            df_conditions = pd.DataFrame(entry_conditions)
            self.analysis_results['entry_conditions'] = df_conditions
            
            print(f"\\nğŸ“Š ENTRY PATTERN ANALYSIS ({len(df_conditions)} signals):")
            print(f"  Average Price Accuracy: {df_conditions['price_accuracy'].mean():.1f}%")
            print(f"  Signals Above MA5: {(df_conditions['above_ma5']).sum()}/{len(df_conditions)} ({(df_conditions['above_ma5'].mean()*100):.1f}%)")
            print(f"  MA5 Above MA10: {(df_conditions['ma5_above_ma10']).sum()}/{len(df_conditions)} ({(df_conditions['ma5_above_ma10'].mean()*100):.1f}%)")
            print(f"  Average Volume Ratio: {df_conditions['volume_ratio'].mean():.1f}x")
            print(f"  Average 5min Momentum: {df_conditions['momentum_5min'].mean():+.2f}%")
            print(f"  Average Price Range: {df_conditions['price_range'].mean():.2f}%")
            
            # Strategy version analysis
            if 'strategy_version' in df_conditions.columns:
                version_stats = df_conditions['strategy_version'].value_counts()
                print(f"\\nğŸ”¢ Strategy Versions:")
                for version, count in version_stats.head().items():
                    version_data = df_conditions[df_conditions['strategy_version'] == version]
                    avg_accuracy = version_data['price_accuracy'].mean()
                    print(f"  {version}: {count} signals (Avg accuracy: {avg_accuracy:.1f}%)")
        
        return entry_conditions
    
    def analyze_target_methodology(self, sample_size: int = 15):
        """Analyze how targets are calculated"""
        print("\\nğŸ¯ TARGET CALCULATION ANALYSIS")
        print("=" * 50)
        
        if self.signals_df.empty:
            return
        
        sample_signals = self.signals_df.head(sample_size)
        target_analysis = []
        
        for idx, signal in sample_signals.iterrows():
            try:
                entry = float(signal['entry_price'])
                sl = float(signal['stop_loss']) if signal['stop_loss'] else None
                t1 = float(signal['target1']) if signal['target1'] else None
                t2 = float(signal['target2']) if signal['target2'] else None
                t3 = float(signal['target3']) if signal['target3'] else None
                
                if sl and t1 and t2 and t3:
                    # Calculate distances and ratios
                    sl_distance = abs(entry - sl)
                    t1_distance = abs(t1 - entry)
                    t2_distance = abs(t2 - entry)
                    t3_distance = abs(t3 - entry)
                    
                    # Risk/reward ratios
                    rr1 = t1_distance / sl_distance
                    rr2 = t2_distance / sl_distance
                    rr3 = t3_distance / sl_distance
                    
                    # Target progression ratios
                    t2_t1_ratio = t2_distance / t1_distance
                    t3_t2_ratio = t3_distance / t2_distance
                    t3_t1_ratio = t3_distance / t1_distance
                    
                    target_info = {
                        'symbol': signal['symbol'],
                        'entry': entry,
                        'sl_distance_pct': (sl_distance / entry) * 100,
                        't1_distance_pct': (t1_distance / entry) * 100,
                        't2_distance_pct': (t2_distance / entry) * 100,
                        't3_distance_pct': (t3_distance / entry) * 100,
                        'rr1': rr1,
                        'rr2': rr2,
                        'rr3': rr3,
                        't2_t1_ratio': t2_t1_ratio,
                        't3_t2_ratio': t3_t2_ratio,
                        't3_t1_ratio': t3_t1_ratio,
                        'strategy_version': signal.get('strategy_version', 'Unknown')
                    }
                    
                    target_analysis.append(target_info)
                    
                    print(f"  {signal['symbol']:6} | SL: {target_info['sl_distance_pct']:.1f}% | " +
                          f"RR1: {rr1:.1f} | RR2: {rr2:.1f} | RR3: {rr3:.1f}")
                
            except Exception as e:
                continue
        
        if target_analysis:
            df_targets = pd.DataFrame(target_analysis)
            self.analysis_results['target_methodology'] = df_targets
            
            print(f"\\nğŸ“Š TARGET METHODOLOGY PATTERNS ({len(df_targets)} signals):")
            print(f"  Average Stop Loss Distance: {df_targets['sl_distance_pct'].mean():.2f}%")
            print(f"  Average Target 1 Distance: {df_targets['t1_distance_pct'].mean():.2f}%")
            print(f"  Average Target 2 Distance: {df_targets['t2_distance_pct'].mean():.2f}%")
            print(f"  Average Target 3 Distance: {df_targets['t3_distance_pct'].mean():.2f}%")
            
            print(f"\\nâš–ï¸ Risk/Reward Patterns:")
            print(f"  Average R/R Target 1: {df_targets['rr1'].mean():.2f}")
            print(f"  Average R/R Target 2: {df_targets['rr2'].mean():.2f}")
            print(f"  Average R/R Target 3: {df_targets['rr3'].mean():.2f}")
            
            print(f"\\nğŸ“ Target Progression:")
            print(f"  T2/T1 Ratio: {df_targets['t2_t1_ratio'].mean():.2f}")
            print(f"  T3/T2 Ratio: {df_targets['t3_t2_ratio'].mean():.2f}")
            print(f"  T3/T1 Ratio: {df_targets['t3_t1_ratio'].mean():.2f}")
            
            # Look for consistent patterns
            print(f"\\nğŸ” Pattern Detection:")
            
            # Check for round number patterns
            rr1_rounded = df_targets['rr1'].round(1)
            common_rr1 = rr1_rounded.mode()
            if not common_rr1.empty:
                print(f"  Most common RR1: {common_rr1.iloc[0]} ({(rr1_rounded == common_rr1.iloc[0]).sum()}/{len(df_targets)} signals)")
            
            # Check target progression consistency
            t2_t1_consistency = (df_targets['t2_t1_ratio'].round(1) == df_targets['t2_t1_ratio'].round(1).mode().iloc[0]).sum()
            print(f"  T2/T1 ratio consistency: {t2_t1_consistency}/{len(df_targets)} signals")
        
        return target_analysis
    
    def analyze_signal_timing(self, sample_size: int = 20):
        """Analyze when signals are typically generated"""
        print("\\nâ° SIGNAL TIMING ANALYSIS")
        print("=" * 50)
        
        if self.signals_df.empty:
            return
        
        # Convert timestamps
        self.signals_df['signal_datetime'] = pd.to_datetime(self.signals_df['timestamp'])
        self.signals_df['hour'] = self.signals_df['signal_datetime'].dt.hour
        self.signals_df['day_of_week'] = self.signals_df['signal_datetime'].dt.day_name()
        self.signals_df['minute'] = self.signals_df['signal_datetime'].dt.minute
        
        print("ğŸ• Signal Distribution by Hour:")
        hourly_dist = self.signals_df['hour'].value_counts().sort_index()
        for hour, count in hourly_dist.head(10).items():
            print(f"  {hour:02d}:00 - {count:3} signals")
        
        print("\\nğŸ“… Signal Distribution by Day:")
        daily_dist = self.signals_df['day_of_week'].value_counts()
        for day, count in daily_dist.items():
            print(f"  {day:9} - {count:3} signals")
        
        print("\\nâ²ï¸ Signal Distribution by Minute:")
        minute_dist = self.signals_df['minute'].value_counts().sort_index()
        common_minutes = minute_dist.head(10)
        for minute, count in common_minutes.items():
            print(f"  :{minute:02d} - {count:3} signals")
        
        # Look for patterns
        print("\\nğŸ” Timing Patterns:")
        
        # Check for exact minute patterns
        exact_minutes = [0, 15, 30, 45]  # Common trading times
        exact_minute_signals = self.signals_df[self.signals_df['minute'].isin(exact_minutes)]
        print(f"  Signals at exact times (0,15,30,45): {len(exact_minute_signals)}/{len(self.signals_df)} ({len(exact_minute_signals)/len(self.signals_df)*100:.1f}%)")
        
        # Check for timeframe alignment
        if 'timeframe' in self.signals_df.columns:
            timeframe_dist = self.signals_df['timeframe'].value_counts()
            print(f"\\nğŸ“Š Timeframe Distribution:")
            for tf, count in timeframe_dist.items():
                print(f"  {tf:4} - {count:3} signals")
    
    def reverse_engineer_algorithm(self):
        """Attempt to reverse engineer the signal generation algorithm"""
        print("\\nğŸ§® ALGORITHM REVERSE ENGINEERING")
        print("=" * 50)
        
        print("ğŸ” Analyzing signal generation patterns...")
        
        # Combine all analysis results
        insights = []
        
        if 'entry_conditions' in self.analysis_results:
            entry_df = self.analysis_results['entry_conditions']
            
            # Price accuracy insight
            avg_accuracy = entry_df['price_accuracy'].mean()
            if avg_accuracy > 95:
                insights.append("ğŸ“ High price accuracy suggests signals are generated close to market time")
            elif avg_accuracy > 90:
                insights.append("ğŸ“ Good price accuracy suggests signals use recent market data")
            else:
                insights.append("ğŸ“ Lower price accuracy suggests signals may be pre-calculated")
            
            # Technical indicator patterns
            ma_usage = entry_df['above_ma5'].mean()
            if ma_usage > 0.7:
                insights.append("ğŸ“ˆ Strong correlation with MA5 suggests moving average-based signals")
            
            volume_ratio = entry_df['volume_ratio'].mean()
            if volume_ratio > 1.5:
                insights.append("ğŸ“Š High volume ratio suggests volume-based signal triggers")
        
        if 'target_methodology' in self.analysis_results:
            target_df = self.analysis_results['target_methodology']
            
            # Risk/reward consistency
            rr1_std = target_df['rr1'].std()
            if rr1_std < 0.2:
                insights.append(f"âš–ï¸ Consistent R/R ratios (Ïƒ={rr1_std:.2f}) suggest algorithmic target calculation")
            
            # Target progression
            avg_progression = target_df['t2_t1_ratio'].mean()
            progression_std = target_df['t2_t1_ratio'].std()
            if progression_std < 0.3:
                insights.append(f"ğŸ“ Consistent target progression ({avg_progression:.2f}Â±{progression_std:.2f}) indicates systematic approach")
        
        # Strategy versions analysis
        if not self.signals_df.empty and 'strategy_version' in self.signals_df.columns:
            versions = self.signals_df['strategy_version'].value_counts()
            print(f"\\nğŸ”¢ Strategy Version Analysis:")
            for version, count in versions.head().items():
                print(f"  Version {version}: {count} signals ({count/len(self.signals_df)*100:.1f}%)")
            
            if len(versions) > 1:
                insights.append(f"ğŸ”„ Multiple strategy versions ({len(versions)}) suggest algorithm evolution/testing")
        
        print("\\nğŸ§  ALGORITHM INSIGHTS:")
        for i, insight in enumerate(insights, 1):
            print(f"{i}. {insight}")
        
        # Hypothetical algorithm structure
        print("\\nğŸ—ï¸ HYPOTHETICAL ALGORITHM STRUCTURE:")
        print("1. ğŸ“Š Market Data Input: Real-time price, volume, moving averages")
        print("2. ğŸ¯ Signal Generation: Likely combination of:")
        print("   - Price vs Moving Average crossover")
        print("   - Volume spike detection")
        print("   - Momentum/trend confirmation")
        print("3. ğŸ’° Target Calculation: Systematic R/R ratios")
        print("   - Target 1: ~1.0-1.5x risk")
        print("   - Target 2: ~2.0-2.5x risk")
        print("   - Target 3: ~3.0-4.0x risk")
        print("4. â° Timing: Signals generated during active trading hours")
        print("5. ğŸ”¢ Version Control: Algorithm updates reflected in version numbers")
        
        return insights
    
    def export_methodology_report(self):
        """Export methodology investigation report"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"data/analysis/methodology_investigation_{timestamp}.json"
        
        os.makedirs("data/analysis", exist_ok=True)
        
        # Compile all analysis results
        report = {
            'timestamp': timestamp,
            'total_signals_analyzed': len(self.signals_df),
            'analysis_results': {}
        }
        
        # Convert DataFrames to dictionaries for JSON serialization
        for key, value in self.analysis_results.items():
            if isinstance(value, pd.DataFrame):
                report['analysis_results'][key] = value.to_dict('records')
            else:
                report['analysis_results'][key] = value
        
        # Add signal timing analysis
        if not self.signals_df.empty:
            report['timing_analysis'] = {
                'hourly_distribution': self.signals_df['hour'].value_counts().to_dict(),
                'daily_distribution': self.signals_df['day_of_week'].value_counts().to_dict(),
                'minute_distribution': self.signals_df['minute'].value_counts().to_dict()
            }
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\\nğŸ“„ Methodology report saved to: {report_file}")
        return report_file

def run_methodology_investigation():
    """Run complete methodology investigation"""
    print("ğŸ”¬ META SIGNALS METHODOLOGY INVESTIGATION")
    print("=" * 60)
    
    investigator = SignalMethodologyInvestigator()
    
    if investigator.signals_df.empty:
        print("âŒ No signals found for analysis.")
        return
    
    # Run all investigation modules
    investigator.analyze_entry_conditions(20)
    investigator.analyze_target_methodology(15)
    investigator.analyze_signal_timing()
    investigator.reverse_engineer_algorithm()
    
    # Export report
    investigator.export_methodology_report()
    
    print("\\nâœ… Methodology investigation complete!")

if __name__ == "__main__":
    run_methodology_investigation()